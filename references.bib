@Manual{example,
    title = {example referance},
    author = {Firstname Lastname},
    year = {1900},
}

@article{Kunc2024,
  author    = {V. Kunc and J. Kléma},
  title     = {Three Decades of Activations: A Comprehensive Survey of 400 Activation Functions for Neural Networks},
  journal   = {arXiv},
  year      = {2024},
  volume    = {arXiv:2402.09092},
  doi       = {10.48550/ARXIV.2402.09092}
}

@article{Vaswani2017,
  author    = {A. Vaswani and Others},
  title     = {Attention Is All You Need},
  journal   = {arXiv},
  year      = {2017},
  month     = {Dec},
  day       = {06},
  volume    = {arXiv:1706.03762v5},
  url       = {http://arxiv.org/abs/1706.03762v5}
}

@article{Lee2023,
  author    = {M. Lee},
  title     = {GELU Activation Function in Deep Learning: A Comprehensive Mathematical Analysis and Performance},
  journal   = {arXiv},
  year      = {2023},
  month     = {Aug},
  day       = {01},
  volume    = {arXiv:2305.12073},
  doi       = {10.48550/arXiv.2305.12073}
}

@article{Shazeer2020,
  author    = {N. Shazeer},
  title     = {GLU Variants Improve Transformer},
  journal   = {arXiv},
  year      = {2020},
  month     = {Feb},
  day       = {12},
  volume    = {arXiv:2002.05202},
  doi       = {10.48550/arXiv.2002.05202}
}

@article{Samuel2023,
  author    = {D. Samuel and A. Kutuzov and L. Øvrelid and E. Velldal},
  title     = {Trained on 100 million words and still in shape: BERT meets British National Corpus},
  journal   = {arXiv},
  year      = {2023},
  month     = {May},
  day       = {05},
  volume    = {arXiv:2303.09859},
  doi       = {10.48550/arXiv.2303.09859}
}

@article{Liu2024,
  author    = {Z. Liu and Others},
  title     = {KAN: Kolmogorov-Arnold Networks},
  journal   = {arXiv},
  year      = {2024},
  month     = {May},
  day       = {02},
  volume    = {arXiv:2404.19756},
  doi       = {10.48550/arXiv.2404.19756}
}

@misc{Rajanand,
  author    = {A. Rajanand and P. Singh},
  title     = {ErfReLU: Adaptive Activation Function for Deep Neural Network}
}

@article{Mirzadeh2023,
  author    = {I. Mirzadeh and Others},
  title     = {ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models},
  journal   = {arXiv},
  year      = {2023},
  month     = {Oct},
  day       = {06},
  volume    = {arXiv:2310.04564},
  doi       = {10.48550/arXiv.2310.04564}
}

@article{Villalobos2022,
  author    = {P. Villalobos and J. Sevilla and L. Heim and T. Besiroglu and M. Hobbhahn and A. Ho},
  title     = {Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning},
  journal   = {arXiv},
  year      = {2022},
  month     = {Oct},
  day       = {25},
  volume    = {arXiv:2211.04325},
  url       = {http://arxiv.org/abs/2211.04325}
}

@article{Hendrycks2023,
  author    = {D. Hendrycks and K. Gimpel},
  title     = {Gaussian Error Linear Units (GELUs)},
  journal   = {arXiv},
  year      = {2023},
  month     = {Jun.},
  day       = {05},
  volume    = {arXiv:1606.08415},
  doi       = {10.48550/arXiv.1606.08415}
}

@article{Ramachandran2017,
  author    = {P. Ramachandran and B. Zoph and Q. V. Le},
  title     = {Searching for Activation Functions},
  journal   = {arXiv},
  year      = {2017},
  month     = {Oct.},
  day       = {16},
  volume    = {arXiv:1710.05941},
  doi       = {10.48550/arXiv.1710.05941}
}

@misc{ltg-bert,
  author       = {LTG Oslo},
  title        = {LTG BERT},
  year         = 2021,
  url          = {https://github.com/ltgoslo/ltg-bert/blob/main/training/model.py#L14},
  note         = {Accessed: 2024-05-15}
}

@misc{efficient-kan,
  author       = {Blealtan},
  title        = {Efficient-KAN},
  year         = 2024,
  url          = {https://github.com/Blealtan/efficient-kan},
  note         = {Accessed: 2024-06-03}
}

@article{Warstadt2023,
  author    = {A. Warstadt and L. Choshen and A. Mueller and A. Williams and E. Wilcox and C. Zhuang},
  title     = {Call for Papers -- The BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus},
  journal   = {arXiv},
  year      = {2023},
  month     = {January},
  day       = {27},
  url       = {http://arxiv.org/abs/2301.11796},
  note      = {Accessed: Apr. 15, 2024}
}

@article{Eldan2023,
  author    = {R. Eldan and Y. Li},
  title     = {TinyStories: How Small Can Language Models Be and Still Speak Coherent English?},
  journal   = {arXiv},
  year      = {2023},
  month     = {May},
  day       = {24},
  url       = {http://arxiv.org/abs/2305.07759},
  note      = {Accessed: Nov. 01, 2023}
}

@misc{huggingfaceNEO,
  author    = {Hugging Face},
  title     = {GPT Neo},
  year      = {2024},
  url       = {https://huggingface.co/docs/transformers/en/model_doc/gpt_neo},
  note      = {Accessed: Jun. 03, 2024}
}

@misc{huggingfaceRoberta,
  author    = {Hugging Face},
  title     = {RoBERTa},
  year      = {2024},
  url       = {https://huggingface.co/docs/transformers/en/model_doc/roberta},
  note      = {Accessed: Jun. 03, 2024}
}

@inproceedings{warstadt-etal-2023-findings,
    title = "Findings of the {B}aby{LM} Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora",
    author = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    editor = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-babylm.1",
    doi = "10.18653/v1/2023.conll-babylm.1",
    pages = "1--34",
}

@article{Warstadt2023blimp,
  author    = {A. Warstadt and others},
  title     = {BLiMP: The Benchmark of Linguistic Minimal Pairs for English},
  journal   = {arXiv},
  year      = {2023},
  month     = {February},
  day       = {14},
  url       = {http://arxiv.org/abs/1912.00582},
  note      = {Accessed: Jan. 22, 2024}
}

@article{Wang2019,
  author    = {A. Wang and A. Singh and J. Michael and F. Hill and O. Levy and S. R. Bowman},
  title     = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  journal   = {arXiv},
  year      = {2019},
  month     = {February},
  day       = {22},
  url       = {http://arxiv.org/abs/1804.07461},
  note      = {Accessed: Jan. 25, 2024}
}

@article{Wang2020,
  author    = {A. Wang and others},
  title     = {SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  journal   = {arXiv},
  year      = {2020},
  month     = {February},
  day       = {12},
  doi       = {10.48550/arXiv.1905.00537}
}

@inproceedings{dror2018hitchhikers,
  title={The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing},
  author={Dror, Rotem and Baumer, Gil and Shlomov, Segev and Reichart, Roi},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  editor={Gurevych, Iryna and Miyao, Yusuke},
  pages={1383--1392},
  year={2018},
  organization={Association for Computational Linguistics},
  publisher={Association for Computational Linguistics},
  address={Melbourne, Australia},
  month={Jul},
  doi={10.18653/v1/P18-1128}
}

@misc{knaw2018integrity,
  author       = {KNAW and NFU and NWO and TO2-Federatie and Vereniging Hogescholen and VSNU},
  title        = {Nederlandse gedragscode wetenschappelijke integriteit},
  year         = {2018},
  publisher    = {Data Archiving and Networked Services (DANS)},
  doi          = {10.17026/DANS-2CJ-NVWU}
}

@article{schaeffer2023pretraining,
  author       = {Rylan Schaeffer},
  title        = {Pretraining on the Test Set Is All You Need},
  year         = {2023},
  journal   = {arXiv},
  archivePrefix = {arXiv},
  eprint       = {2309.08632},
  primaryClass = {cs.CL},
  month        = sep,
  note         = {doi: 10.48550/arXiv.2309.08632},
  url          = {https://doi.org/10.48550/arXiv.2309.08632}
}

@inproceedings{nair2010rectified,
  title={Rectified Linear Units Improve Restricted Boltzmann Machines},
  author={Nair, Vinod and Hinton, Geoffrey},
  booktitle={Proceedings of the 27th International Conference on Machine Learning (ICML)},
  volume={27},
  year={2010},
  pages={814}
}

@article{dubey2022activation,
  title={Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark},
  author={Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, B. B.},
  journal={arXiv preprint arXiv:2109.14545},
  year={2022},
  month={Jun},
  doi={10.48550/arXiv.2109.14545}
}


@misc{chaudhuri_b-splines_2021,
	title = {B-Splines},
	url = {http://arxiv.org/abs/2108.06617},
	doi = {10.48550/arXiv.2108.06617},
	abstract = {{BSplines} are one of the most promising curves in computer graphics. They are blessed with some superior geometric properties which make them an ideal candidate for several applications in computer aided design industry. In this article, some basic properties of B-Spline curves are presented. Two significant B-Spline properties viz convex hull property and repeated points effects are discussed. The {BSplines} computation in computational devices is also illustrated. An industry application based on image processing where B-Spline curve reconstructs the 3D surfaces for {CT} image datasets of inner organs further highlights the strength of these curves},
	number = {{arXiv}:2108.06617},
	publisher = {{arXiv}},
	author = {Chaudhuri, Arindam},
	urldate = {2024-06-11},
	date = {2021-08-14},
	eprinttype = {arxiv},
	eprint = {2108.06617 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{geva_transformer_2022,
	title = {Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space},
	url = {http://arxiv.org/abs/2203.14680},
	doi = {10.48550/arXiv.2203.14680},
	abstract = {Transformer-based language models ({LMs}) are at the core of modern {NLP}, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network ({FFN}) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each {FFN} layer as an additive update to that distribution. Then, we analyze the {FFN} updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single {FFN} parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling {LM} predictions, where we reduce the toxicity of {GPT}2 by almost 50\%, and for improving computation efficiency with a simple early exit rule, saving 20\% of computation on average.},
	number = {{arXiv}:2203.14680},
	publisher = {{arXiv}},
	author = {Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
	urldate = {2024-06-11},
	date = {2022-10-12},
	eprinttype = {arxiv},
	eprint = {2203.14680 [cs]},
	keywords = {Computer Science - Computation and Language},
}