\begin{abstract} % PRESENT TENSE!!!
    The rapid expansion of large language models (LLMs) driven by the transformer architecture has introduced concerns about the lack of high-quality training data. This study investigates the role of activation functions in smaller-scale language models, specifically those with approximately 10million (M) parameters, to ensure sustained progress in LLM development despite data limitations. Activation functions, crucial for neural network performance, have evolved significantly, but comprehensive comparisons under consistent conditions remain scarce, especially for smaller parameter count models. This research systematically evaluates traditional and novel activation functions, including learnable variants, and introduces the Kolmogorov-Arnold Network (KAN) to language modeling. Using Hugging Face implementations of GPT-Neo and RoBERTa models, this study assesses performance impacts through the BabyLM evaluation pipeline. 
    TODO: Add results and conclusions.
\end{abstract}