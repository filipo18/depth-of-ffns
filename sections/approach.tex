\clearpage
\section{Approach}

% ** How am i able to do this **
% - transfomers from layers
% - activation functions are used in FFN layer which is implementation of MLP

% ** Motivate choices of activation functions **
% - to compare activation functions I will take existing Hugging face implementation of GPT-NEO and roBERTa and replace the activation function with the one I want to test
% - according to \cite{Mirzadeh2023} I want to test how much worse ReLU is, then compare with relu with learnable parameters PReLU (explain prelu)
% - according to [reference to swish paper] I want to test swish, to get baseline start with SiLU then compare with swish (explain swish)
% - according to [reference to GeGLU paper] I want to test GeGLU, to get baseline start with GELU then compare with GeGLU (explain GeGlU)
% - parametrize GELU with learnable to comapre against GELU (explain how i parametrized gelu)
% - if all that successfull make parametrized GeGLU
% - lastly to compare KAN network against all of the above 

% ** How to kan **
% - implementation exists by the authors of the paper 
% - seems to be running into a bunch of issues and is very slow
% - using efficient kan impelementation isntead
% - have to decide on some paratmees grid: int the number of grid intervals and k: the order of piecewise polynomial, will use 3 and 5 based on the paper 

Transformers are composed of multiple layers, each playing a crucial role in processing input data and generating meaningful representations. Among these layers, the Feed-Forward Neural Network (FFN) layer typically consists of two linear transformations with an activation function in between, effectively forming a simple Multi-Layer Perceptron (MLP). This structure allows the default activation function to be switched out with different activation functions for testing, enabling a direct comparison of their performance while keeping everything else the same. To explore the effectiveness of various activation functions, I will modify existing implementations of prominent models like GPT-NEO and RoBERTa from the Hugging Face library

\subsection{Choosing activation functions}
% The activation functions that will be evaluated are the following: ReLU, SiLU, Swish, PReLU, GELU, GEGLU, learnable GELU and learnable GEGLU. Additionaly the KAN network will be compared against all of those options. 
The activation functions to be evaluated are: ReLU, SiLU, Swish, PReLU, GELU, GEGLU, learnable GELU, and learnable GEGLU. Additionally, the KAN network will be compared against all of these options.

\subsubsection{GELU}
Currently, the most popular activation function in LLMs is also used as the default activation function in the baseline models GPT-NEO and roBERTa. It is a smooth approximation of ReLU, originally defined as \(\text{GELU}(x) = x \cdot \Phi(x)\), where \(\Phi(x)\) is the Cumulative Distribution Function for the Gaussian Distribution. or optimization purposes, since calculating \(\Phi(x)\) is computatinally expensive, it is instead calculated with the tanh approximation as:
\(\text{GELU}(x) = 0.5x \left(1 + \tanh\left(\sqrt{\frac{2}{\pi}} \left(x + 0.044715x^3\right)\right)\right)\) \cite{Hendrycks2023}. This function will be used as a baseline for comparison with all other activations.
[add figure of GELU]

\subsubsection{ReLU and PReLU}
\textit{ReLU} was considered state-of-the-art at the time of the original transformer paper \cite{Vaswani2017}, but has since been surpassed by other activation functions. Baseline models are implemented using the PyTorch library, which prvides an implementation for the ReLU activation function [RELU citation from docs], which will be used in this research. 

\[
\text{ReLU}(x) = \max(0, x)
\]

This implementation will be used as a baseline for comparison with \textit{PReLU} and \textit{GELU}. The comparison with GELU is motivated by the findings of \citet{Mirzadeh2023}, which suggest that the use of ReLU is acceptable as the impact of activation functions diminishes with increasing model size. This research aims to determine how much worse (if at all) ReLU is compared to GELU when used with smaller models.


\textit{PReLU} is a variant of ReLU that includes learnable parameters. This allows the activation function to learn the optimal slope for negative values. The PReLU function is defined as \[\text{PReLU}(x) = \max(0, x) + a \min(0, x)\]
[add figure of ReLU]

\subsubsection{SiLU and Swish}

\subsubsection{GELU and Learnable GELU}

\subsubsection{GEGLU and Learnable geglu}

Motivate choices of activation functions
To explore the effectiveness of various activation functions, I will modify existing implementations of prominent models like GPT-NEO and RoBERTa from the Hugging Face library. According to Mirzadeh et al. (2023), I am interested in evaluating how much worse ReLU performs compared to PReLU, which includes learnable parameters. Additionally, I will test the Swish activation function by starting with SiLU (a baseline version of Swish) and then comparing it to Swish. Furthermore, I will investigate GeGLU by comparing it to the standard GELU activation function. To thoroughly understand the impact, I will also parameterize GELU with learnable components and benchmark it against its standard form. Finally, I will compare these findings against the performance of the KAN network to establish a comprehensive understanding.

The implementation of KAN, as described by the authors of the paper, is available but has shown to be problematic and slow. To address these issues, I will utilize an efficient KAN implementation. This approach requires careful selection of certain parameters, specifically the number of grid intervals (int) and the order of piecewise polynomials (k). Based on recommendations from the paper, I will set these parameters to 3 and 5, respectively. This configuration aims to balance performance and computational efficiency, ensuring that the KAN implementation is both effective and practical for the experiments.