

% ** How am i able to do this **
% - transfomers from layers
% - activation functions are used in FFN layer which is implementation of MLP

% ** Motivate choices of activation functions **
% - to compare activation functions I will take existing Hugging face implementation of GPT-NEO and roBERTa and replace the activation function with the one I want to test
% - according to \cite{Mirzadeh2023} I want to test how much worse ReLU is, then compare with relu with learnable parameters PReLU (explain prelu)
% - according to [reference to swish paper] I want to test swish, to get baseline start with SiLU then compare with swish (explain swish)
% - according to [reference to GeGLU paper] I want to test GeGLU, to get baseline start with GELU then compare with GeGLU (explain GeGlU)
% - parametrize GELU with learnable to comapre against GELU (explain how i parametrized gelu)
% - if all that successfull make parametrized GeGLU
% - lastly to compare KAN network against all of the above 

% ** How to kan **
% - implementation exists by the authors of the paper 
% - seems to be running into a bunch of issues and is very slow
% - using efficient kan impelementation isntead
% - have to decide on some paratmees grid: int the number of grid intervals and k: the order of piecewise polynomial, will use 3 and 5 based on the paper 
\section{Approach} % PAST TENSE!!!
\label{sec:approach}
The transfomer structure (as introduced in section \ref{sec:background}) allows the default activation function to be switched out with different activation functions for testing, enabling a direct comparison of their performance while keeping the rest of the architecture the same. To explore the effectiveness of various activation functions, we will modify the existing Hugging face implementations of \textsc{GPTNeo} \cite{huggingfaceNEO} and \textsc{RoBERTa} \cite{huggingfaceRoberta}.

\subsection{Choosing activation functions}
% The activation functions that will be evaluated are the following: ReLU, SiLU, Swish, PReLU, GELU, GEGLU, learnable GELU and learnable GEGLU. Additionaly the KAN network will be compared against all of those options. 
The activation functions evaluated were Rectified Linear Unit (\textsc{ReLU}) \footnote{PyTorch Library, "torch.nn.ReLU," \textit{PyTorch Documentation}, https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html}, Sigmoid Linear Unit (\textsc{SiLU}) \cite{Hendrycks2023}, Gated SiLU with learnable parameters (\textsc{Swish}) \cite{eger_is_2019}, Parametric ReLU (\textsc{PReLU}) \footnote{PyTorch Library, \textit{torch.nn.ReLU}, PyTorch Documentation, \url{https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html}.}, Gaussian Error Linear Unit (\textsc{GELU}) (basline models), \textsc{Adaptable GELU} and \textsc{No Activation Function}. Additionally, the Kolmogorov–Arnold Networks \textsc{KAN network} \cite{Liu2024} will be compared against all of these options.

\subsubsection{GELU}
Currently, the most popular activation function in LLMs is also used as the default activation function in the baseline models \textsc{GPT-NEO} and \textsc{roBERTa}. It is a smooth approximation of \textsc{ReLU}, originally defined as \(\text{GELU}(x) = x \cdot \Phi(x)\), where \(\Phi(x)\)
is the Cumulative Distribution Function for the Gaussian Distribution. For optimization purposes, since calculating \(\Phi(x)\) is computationally expensive, it is instead calculated with the \textsc{tanh} approximation as:

\(\text{GELU}(x) = 0.5x \left(1 + \tanh\left(\sqrt{\frac{2}{\pi}} \left(x + 0.044715x^3\right)\right)\right)\) \cite{Hendrycks2023}. 

This function will serve as the baseline for comparing all other activation functions, with a particular focus on evaluating it against the novel Learnable GELU.

\subsubsection{ReLU and PReLU}
\textsc{ReLU} was considered state-of-the-art at the time of the original transformer paper \cite{Vaswani2017}, but has since been surpassed by other activation functions.

\[
\text{ReLU}(x) = \max(0, x)
\]

\textsc{ReLU} will be used for comparison with \textsc{PReLU} and \textsc{GELU}. The comparison with \textsc{GELU} is motivated by the findings of \textit{I. Mirzadeh et al.} \cite{Mirzadeh2023}, which suggest that the use of ReLU is acceptable as the impact of activation functions diminishes with increasing model size. The objective is to assess the extent to which \textsc{ReLU} underperforms compared to \textsc{GELU} when applied to smaller models.

\textsc{PReLU} is a variant of \textsc{ReLU} that incorporates learnable parameters, allowing the activation function to adaptably learn the optimal slope for negative values.
\[
\text{PReLU}(x) = \max(0, x) + a \min(0, x)
\]

where a is a learnable parameter. This introduces x learnable parameters where x is the FFNs’ intermediate size in the transformer. The objective is to evaluate whether adding a learnable parameter to \textsc{ReLU} can enhance performance or increase the training time.

\subsubsection{SiLU and Swish}
\textsc{SiLU} was evaluated on LLMs in the original \textsc{GELU} paper \cite{Hendrycks2023} but was found to perform worse than the \textsc{GELU} activation function. It is defined as 
\[
\text{SiLu}(x) = x \cdot \sigma(x), \text{ where } \sigma(x) \text{ is the logistic sigmoid.}
\]
It will be used only as a baseline comparison for the \textit{Swish} activation function, which is its adaptable counterpart with learnable parameters, aiding the objective of exploring the impact of adding learnable parameters to activation functions. 

\textit{Swish} is a self-gated activation function that was proposed by \textit{Ramachandran et al.} \cite{Ramachandran2017}. It was implemented as proposed in a paper
\[
    \text{swish}(x) = x \cdot \text{silu}(\alpha \cdot x)
\]
where \(\alpha\) is a learnable parameter. The objective is to evaluate whether the \textsc{Swish} activation function outperforms the non adaptable \textsc{SiLU} to evaluate the impact of adding learnable parameters to activation functions.

\subsubsection{Adaptable GELU}
The GELU activation function will be parameterized with learnable parameters. The implementation will be based on the PyTorch GELU implementation[cite] with \textsc{tanh} approximation, which, out of the box, does not support learnable parameters. This approach appears to be novel, as no prior research has been found that explores the impact of adding learnable parameters to the \textsc{GELU} activation function. The new \textsc{GELU} activation function adds a learnable parameter as a scaling factor and is defined as follows:

\[
    0.5 \cdot \alpha \cdot \text{x} \cdot \left( 1.0 + \tanh \left( \sqrt{\frac{2.0}{\pi}} \cdot (\text{x} + 0.044715 \cdot \text{x}^3) \right) \right)
\]

where \(\alpha\) is a learnable parameter. 

This activation functions adds a total of 2048 learnable parameters. The objective is to evaluate whether the \textsc{Adaptable GELU} activation function outperforms the standard \textsc{GELU} activation function to evaluate the impact of adding learnable parameters to activation functions.

% \subsubsection{GEGLU and Learnable GEGLU}
% The \textit{GEGLU} activation function is a variant of the GELU activation function that incorporates a gating mechanism as proposed by \textit{Shazeer et al.} \cite{Shazeer2020}. It promises to improve the performance of the GELU activation function by adding a gating mechanism. The GEGLU activation function was implemented as used by 2023 BabyLM winner \cite{Samuel2023} \cite{ltg-bert}:
% \begin{lstlisting}[language=Python, caption={Implementation of GeGLU}]
%     class GeGLU(nn.Module):
%      def forward(self, x):
%          x, gate = x.chunk(2, dim=-1)
%          x = x * F.gelu(gate, approximate='tanh')
%          return x
% \end{lstlisting}

% The \textit{Learnable GEGLU} activation function is an enhanced variant of the GEGLU activation function, incorporating a learnable parameter following the same idea as Learnable GEGLU. This implementation extends the standard GEGLU by introducing a learnable scaling factor, making it a novel approach. The new Learnable GEGLU activation function is defined as follows:

% \begin{lstlisting}[language=Python, caption={Implementation of Learnable GeGLU}]
%     def forward(self, input: Tensor) -> Tensor:
%         x, gate = input.chunk(2, dim=-1)
%         x = x * 0.5 * self.beta * gate * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (gate + 0.044715 * torch.pow(gate, 3.0))))
%         return x
% \end{lstlisting}

% The objective is to first evaluate the performance of GeGLU compared to GELU, and then to evaluate the performance of Learnable GeGLU compared to GeGLU. The hypothesis is that Learnable GeGLU will be the best-performing activation function.

\subsubsection{No Activation Function}
The \textsc{No Activation Function} will be used as a baseline for comparison with all other activation functions. This baseline will help determine the impact of using an activation function compared to not using one.

\subsubsection{KAN-Network}
The \textsc{KAN network} is a novel activation function that has shown promising results in the literature. The implementation of the \textsc{KAN network} is available by original authors, but has shown to be problematic and slow during our research. Those issues were addressed in efficient-KAN\footnote{\label{footnote:efficient-kan}Blealtan, \textit{Efficient-KAN, GitHub Repository}, 2024. Available at: \url{https://github.com/Blealtan/efficient-kan} (Accessed: 2024-06-03).} implementation, which was the most popular optimization on GitHub of the original KAN paper at the time of this study. This approach requires careful selection of certain parameters, specifically the number of grid intervals and the order of piecewise polynomials. Based on recommendations from the paper, we will set these parameters to 3 and 5, respectively. The configuration aims to balance performance and computational efficiency, ensuring that the KAN implementation is both effective and practical for the experiments. FFN layers from GPT-NEO and roBERTa both use MLPs in their implementation, which can be directly replaced with efficient-KAN implementation. As KAN network increases the amount of total trainable parameters in FFn from 2M to slightly over 8M, the intermediate size for models with \textsc{Kan netowrk} was decreased to 256 to keep the number of parameters in FFN around 2M.
The objective is to evaluate the performance of the KAN network compared to the other activation functions and to determine whether it is a viable alternative for MLPs in LLMs. Additionally, the \textsc{KAN network} offers the unique capability to extract symbolic representations of the learned activation functions from splines. It also provides the ability to retrain or further train specific parts of the network if necessary. Unfortunately, due to time constraints and the current implementation, this aspect will not be explored in this study. For further discussion, see Section \ref{sec:discussion}.