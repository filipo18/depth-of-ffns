\section{Approach}

% ** How am i able to do this **
% - transfomers from layers
% - activation functions are used in FFN layer which is implementation of MLP

% ** Motivate choices of activation functions **
% - to compare activation functions I will take existing Hugging face implementation of GPT-NEO and roBERTa and replace the activation function with the one I want to test
% - according to \cite{Mirzadeh2023} I want to test how much worse ReLU is, then compare with relu with learnable parameters PReLU (explain prelu)
% - according to [reference to swish paper] I want to test swish, to get baseline start with SiLU then compare with swish (explain swish)
% - according to [reference to GeGLU paper] I want to test GeGLU, to get baseline start with GELU then compare with GeGLU (explain GeGlU)
% - parametrize GELU with learnable to comapre against GELU (explain how i parametrized gelu)
% - if all that successfull make parametrized GeGLU
% - lastly to compare KAN network against all of the above 

% ** How to kan **
% - implementation exists by the authors of the paper 
% - seems to be running into a bunch of issues and is very slow
% - using efficient kan impelementation isntead
% - have to decide on some paratmees grid: int the number of grid intervals and k: the order of piecewise polynomial, will use 3 and 5 based on the paper 

Transformers comprise multiple layers, each crucial in processing input data and generating meaningful representations. Among these layers, the Feed-Forward Neural Network (FFN) layer typically consists of two linear transformations with an activation function in between, effectively forming a simple Multi-Layer Perceptron (MLP). This structure allows the default activation function to be switched out with different activation functions for testing, enabling a direct comparison of their performance while keeping everything else the same. To explore the effectiveness of various activation functions, I will modify existing implementations of prominent models like GPT-NEO and RoBERTa from the Hugging Face library.

\subsection{Choosing activation functions}
% The activation functions that will be evaluated are the following: ReLU, SiLU, Swish, PReLU, GELU, GEGLU, learnable GELU and learnable GEGLU. Additionaly the KAN network will be compared against all of those options. 
The activation functions to be evaluated are ReLU, SiLU, Swish, PReLU, GELU, GEGLU, learnable GELU, and learnable GEGLU. Additionally, the KAN network will be compared against all of these options.

\subsubsection{GELU}
Currently, the most popular activation function in LLMs is also used as the default activation function in the baseline models GPT-NEO and roBERTa. It is a smooth approximation of ReLU, originally defined as \(\text{GELU}(x) = x \cdot \Phi(x)\), where \(\Phi(x)\) is the Cumulative Distribution Function for the Gaussian Distribution. or optimization purposes, since calculating \(\Phi(x)\) is computationally expensive, it is instead calculated with the tanh approximation as:
\(\text{GELU}(x) = 0.5x \left(1 + \tanh\left(\sqrt{\frac{2}{\pi}} \left(x + 0.044715x^3\right)\right)\right)\) \cite{Hendrycks2023}. This function will be used as a baseline for comparison with all other activations.
[add figure of GELU]

\subsubsection{ReLU and PReLU}
\textit{ReLU} was considered state-of-the-art at the time of the original transformer paper \cite{Vaswani2017}, but has since been surpassed by other activation functions. Baseline models are implemented using the PyTorch library, which provides an implementation for the ReLU activation function [RELU citation from docs], which will be used in this research. 

\[
\text{ReLU}(x) = \max(0, x)
\]

This implementation will be used as a baseline for comparison with \textit{PReLU} and \textit{GELU}. The comparison with GELU is motivated by the findings of \textit{I. Mirzadeh and Others} \cite{Mirzadeh2023}, which suggests that the use of ReLU is acceptable as the impact of activation functions diminishes with increasing model size. The objective is to determine how much worse (if at all) ReLU is compared to GELU when used with smaller models.

\textit{PReLU} is a variant of ReLU that incorporates learnable parameters, allowing the activation function to adaptably learn the optimal slope for negative values. The PReLU function is available in the PyTorch library [pytorch citation]. It is mathematically defined as

\[
\text{PReLU}(x) = \max(0, x) + a \min(0, x)
\]

where a is a learnable parameter. It takes the \textit{num\_parameters} parameter which was set to \textit{intermediate\_size}. The objective is to evaluate whether adding a learnable parameter to ReLU can enhance performance and to measure the impact on training time.
[ADD FIGURE OF RELU]

\subsubsection{SiLU and Swish}
\textit{SiLU} was initially evaluated on LLMs in the original GELU paper \cite{Hendrycks2023} but was found to perform worse than the GELU activation function. It is defined as 
\[
\text{silu}(x) = x \cdot \sigma(x), \text{ where } \sigma(x) \text{ is the logistic sigmoid.}
\]
PyTorch implementation of SiLU was used [cite pytorch silu].
It will be used only as a baseline comparison for the \textit{Swish} activation function, which is its counterpart with learnable parameters, aiding the objective of exploring the impact of adding learnable parameters to activation functions. 

\textit{Swish} is a self-gated activation function that was proposed by \textit{Ramachandran et al.} \cite{Ramachandran2017}. It is not available in the PyTorch library so it was implemented as proposed in the paper 
\[
    \text{swish}(x) = x \cdot F.\text{silu}(\beta \cdot x)
\]
where \(\beta\) is a learnable parameter. Using the idea seen in PyTorch PReLU implementation the \textit{num\_parameters} parameter, which determines the number of learnable parameters was added and set to \textit{intermediate\_size}. The objective is to evaluate whether the Swish activation function can outperform SiLU and to measure the impact of adding learnable parameters to activation functions.

[ADD FIGURE OF SILU]

\subsubsection{Learnable GELU}
The GELU activation function will be parameterized with learnable parameters. The implementation will be based on the PyTorch GELU implementation with tanh approximation, which, out of the box, does not support learnable parameters. This approach appears to be novel, as no prior research has been found that explores the impact of adding learnable parameters to the GELU activation function. The new GELU activation function adds a learnable parameter as a scaling factor and is defined as follows:

\[
    0.5 \cdot \beta \cdot \text{input} \cdot \left( 1.0 + \tanh \left( \sqrt{\frac{2.0}{\pi}} \cdot (\text{input} + 0.044715 \cdot \text{input}^3) \right) \right)
\]

where \(\beta\) is a learnable parameter. 

The \textit{num\_parameters} parameter was set to \textit{intermediate\_size} to determine the number of learnable parameters. The objective is to evaluate whether the learnable GELU activation function can outperform the standard GELU activation function and to measure the impact of adding learnable parameters to activation functions.

\subsubsection{GEGLU and Learnable GEGLU}
The \textit{GEGLU} activation function is a variant of the GELU activation function that incorporates a gating mechanism as proposed by \textit{Shazeer et al.} \cite{Shazeer2020}. It promises to improve the performance of the GELU activation function by adding a gating mechanism. The GEGLU activation function was implemented as used by 2023 BabyLM winner \cite{Samuel2023} \cite{ltg-bert}:
\begin{lstlisting}[language=Python, caption={Implementation of GeGLU}]
    class GeGLU(nn.Module):
     def forward(self, x):
         x, gate = x.chunk(2, dim=-1)
         x = x * F.gelu(gate, approximate='tanh')
         return x
\end{lstlisting}

The \textit{Learnable GEGLU} activation function is an enhanced variant of the GEGLU activation function, incorporating a learnable parameter following the same idea as Learnable GEGLU. This implementation extends the standard GEGLU by introducing a learnable scaling factor, making it a novel approach. The new Learnable GEGLU activation function is defined as follows:

\begin{lstlisting}[language=Python, caption={Implementation of Learnable GeGLU}]
    def forward(self, input: Tensor) -> Tensor:
        x, gate = input.chunk(2, dim=-1)
        x = x * 0.5 * self.beta * gate * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (gate + 0.044715 * torch.pow(gate, 3.0))))
        return x
\end{lstlisting}

The objective is to first evaluate the performance of GeGLU compared to GELU, and then to evaluate the performance of Learnable GeGLU compared to GeGLU. The hypothesis is that Learnable GeGLU will be the best-performing activation function.

\subsubsection{KAN-Network}
The KAN network is a novel activation function that has shown promising results in the literature. The implementation of the KAN network is available but has shown to be problematic and slow. To address these issues, I will utilize an efficient-KAN implementation \cite{efficient-kan}. This approach requires careful selection of certain parameters, specifically the number of grid intervals (int) and the order of piecewise polynomials (k). Based on recommendations from the paper, I will set these parameters to 3 and 5, respectively. This configuration aims to balance performance and computational efficiency, ensuring that the KAN implementation is both effective and practical for the experiments. FFN layers from GPT-NEO and roBERTa both use MLPs in their implementation, which can be directly replaced with efficient-KAN implementation. As KAN network adds additional trainable parameters, the intermediate size for GPT-NEO was decreased to 256 and for roBERTa to 384 to keep the number of parameters around 10M as for other models. 


The objective is to evaluate the performance of the KAN network compared to the other activation functions and to determine whether it is a viable alternative for LLMs.