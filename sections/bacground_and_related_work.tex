% Over the last three decades, researchers have proposed approximately 400 different activation functions \cite{Kunc2024}, suggesting a vast landscape of possibilities for neural network optimization. Historically, models such as those based on the transformer architecture, introduced in the initial transformer paper \cite{Vaswani2017}, predominantly utilized Rectified Linear Unit (ReLU). However, the landscape began shifting when other activation functions started being considered.

% A pivotal moment in the evolution of activation functions in language models was marked by the introduction of the Gaussian Error Linear Unit (GELU)\cite{Lee2023}. GELU has become the popular choice for language models and it's also the default activation function RoBERTa and GPT-Neo, implemented by Hugging Face which are the ones I will be using as my baseline. This function's popularity underscores its perceived utility over traditional functions like ReLU in specific contexts, particularly in models with parameters on the scale of hundreds of millions.

% Yet, as already mentioned, a significant research gap persists in the comparison of activation functions, particularly on models smaller than 100m parameters. This gap could be explained by findings from another paper, which suggests that the impact of activation functions diminishes as the model size increases, evident in models with over a billion parameters \cite{Mirzadeh2023}. This also explains the initial move away from ReLU, since all the research on alternatives was done on models with the size of approximately 100 million parameters.

% Given these insights, this research will explore the impact of various activation functions on smaller-scale language models with around 10 million parameters. The hypothesis posits that at smaller scales, the choice of activation function is crucial, potentially leading to significant performance variations.

% Further, this research will delve into an area of adaptive activation functions. It has been shown that adaptive activation functions outperform static ones in text-to-text machine translation \cite{Rajanand}, but there seems to be a lack of further research into adaptive functions in language models, likely due to an expected tradeoff between additional trainable parameters and impact on performance. Additionally, recent developments in KAN: Kolmogorov-Arnold Networks \cite{Liu2024} suggest a shift towards using activation functions on edges instead of nodes, but due to its recency, it has yet to be tested on a language model. This research will also experiment with this concept and apply it to language modeling to assess its efficacy at smaller scales.

% This paper will structure its discussion starting with a review of historical and current activation functions, followed by methodology, experimental setup, results, and conclusions. By addressing these facets, the study aims to illuminate how different activation functions can enhance or compromise the performance of scaled-down language models, ultimately contributing to the optimization of neural network design.
\section{Background and related work} % PRESENT TENSE!!!
\label{sec:background}
% ** Quick explanation of activation functions ** NOTE: Fix up this paragraph
% - Activation functions are used to introduce non-linearity to neural networks
% - they are on nodes 

% ** Timeline of activations in LLMs **
% - "Attention is all you need" \cite{Vaswani2017} just used the state-of-the-art activation function at the time, ReLU
% - No significant improvements till GELU \cite{Lee2023} was introduced which soon became the default activation function in LLMs
% - Development continues new alternatives presented such as GeGLU \cite{Shazeer2020}

% ** The gap **
% - sigmoid -> (models bigger) -> ReLU -> (models bigger) -> GELU -> (models bigger) -> back to rely
% - as mentioned gap in the comprehensive comparison of activation functions especially in smaller models
% - the gap can be explained by findings from a recent paper that suggests moving back to ReLU as the impact of activation functions diminishes as the model size increases \cite{Mirzadeh2023}
% ADAPTIVE FUNCTIONS GAP
% - another interesting gap is the lack of research on adaptive activation functions in LLMs, only one paper found \cite{Rajanand} which found ... (insert here)
% - A very recent development addressing the performance of smaller models is KAN: Kolmogorov-Arnold Networks \cite{Liu2024} which suggests a shift towards using activation functions on edges instead of nodes

% ** KAN background and gap **
% - KAN is a new type of neural network that uses activation functions on edges instead of nodes
% - It has been shown to outperform traditional neural networks in some tasks \cite{Liu2024}
% - It seems to be mostly suited for scientific applications such as solving partial differential equations \cite{Liu2024}
% - but at the time of my research no tests on LLMs have been done yet
% - the main benefit is optimizing activation on each edge instead of the node, using something called splines 
% - the main drawback is the increased number of trainable parameters

% NOTE: Fix Lee2023 citation for GELU
Transformers comprise multiple layers, each crucial in processing input data and generating meaningful representations. Among these layers, the Feed-Forward Neural Network (FFN) layer typically consists of two linear transformations with an activation function in between, effectively forming a simple Multi-Layer Perceptron (MLP) \cite{geva_transformer_2022}.

Activation functions are used to introduce non-linearity into neural networks, allowing them to model complex relationships in the data. They are applied to the nodes of the network and are essential for enabling the network to learn and perform a wide range of tasks, beyond what linear models can achieve \cite{dubey2022activation}.

The famous paper ``Attention is All You Need“ \cite{Vaswani2017} used the state-of-the-art activation function at the time, Rectified Linear Unit \textsc{ReLU}. Since then, no significant improvements were made until the introduction of Gaussian Error Linear Unit \textsc{GELU}, which quickly became the default activation function in most of the state-of-the-art LLMs. The popularity of GELU stems from its ability to enhance model performance without introducing an efficiency overhead by combining the properties of \textsc{ReLU} and \textsc{Sigmoid} function. It balances between linearity and nonlinearity, allowing it to handle both positive and negative inputs gracefully \cite{Hendrycks2023}. Despite these advancements, continuous innovation leads to alternatives like Gated GELU \textsc{GeGLU}, noted for its effectiveness \cite{Shazeer2020}, also used in last year's winner of the BabyLM challenge \cite{Samuel2023}. However, a recently published paper suggests a return to ReLU \cite{Mirzadeh2023}, further confusing the search for the optimal activation function. Fortunately, it also provides some clues that guide further exploration and motivate this research.

The paper suggests that the impact of activation functions diminishes as the model size increases, evident in models with over a billion parameters \cite{Mirzadeh2023}. This also explains the initial move away from \textsc{ReLU}, since the research on activation alternatives was done on models with the sizes of 100+ million parameters \cite{Shazeer2020} \cite{Hendrycks2023}. Highlighting this finding further motivates the need to investigate activation functions in smaller models. The impact of activation functions is expected to be more significant in smaller models, and until now, decisions have been made with the trend of increasing model size in mind.

Furthermore, another gap appears in the research on activation functions with trainable parameters (adaptable activation functions). The possible explanation for this could be the trade-off between additional trainable parameters and performance. However, this was primarily studied in larger models. To our knowledge the only paper on adaptive activation functions in LLMs is by Rajanand et al. \cite{Rajanand}, which found that adaptive activation functions outperform static ones in text-to-text machine translation, a task also performed by language models. This suggests that adaptive activation functions could be beneficial for smaller models, but further research is needed to confirm this hypothesis. Given these insights, this research will explore the impact of various activation functions on smaller-scale language models with around 10 million parameters. Hypothesizing that at smaller scales, the choice of activation function is crucial, having learnable parameters could be beneficial, while the added parameters from the function's parametrization remain relatively minimal compared to the total model size.

Kolmogorov-Arnold Networks (KAN) represent a recent development in neural network architecture, where activation functions are applied on edges instead of nodes \cite{Liu2024}. This approach has been shown to outperform traditional neural networks in some tasks, particularly in scientific applications such as solving partial differential equations. However, at the time of this study, it has yet to be tested on language models. The primary benefit of KAN is the optimization of activation on each edge using splines. A spline is a piece-wise-defined polynomial function used in interpolation and approximation to create smooth curves through a set of points \cite{chaudhuri_b-splines_2021}. With a spline on each edge (see figure \ref{fig:kan}), each edge can have its own custom activation function, trained separately and uniquely shaped. In contrast, adaptive activation functions have the same shape but different gradients. However, this comes with the drawback of an increased number of trainable parameters. This research will experiment with applying KAN to language modeling to assess its efficacy at smaller scales, addressing the gap in the current literature.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\columnwidth * 2]{figures/kan-network.png}
    \caption{KAN vs MLP Z. Liu et al., “KAN: Kolmogorov-Arnold Networks.” arXiv, May 02, 2024. doi: 10.48550/arXiv.2404.19756.}
    \label{fig:kan}
\end{figure*}