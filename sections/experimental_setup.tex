% Define the research questions clearly. Discuss the specific configurations you have settled on, and
% elaborate on the data sets you have utilized. Document the evaluation setup and define the metrics.
% ● 4.1 Research Questions
% ● 4.2 Dataset
% ● 4.3 Models
% ● 4.4 Evaluation Setting and Metrics.
% Evaluation Metrics: You should mention the components of the BabyLM evaluation pipeline; but can defer
% explaining each (fine-tune) task in detail to the BLiMP/GLUE/SuperGLUE papers. However, you should
% describe the process each of these components uses to return a score (i.e. how is the final score
% computed?)
% Evaluation Settings: hardware.
% ● 4.5 Configuration and Implementation Details

\section{Experimental setup} % PAST TENSE!!!
\label{sec:experimental_setup}

\subsection{Research questions}
Through the experiment, we aim to answer the following research questions:
\begin{itemize}
    \item \textit{Is the choice of activation function relevant to the performance of smaller models with 10M parameters?}  We compared the pre-trained baseline models and models with \textit{SiLU} and \textit{ReLU} activation functions and compared the results of evaluation on BabyLM evaluation pipeline \cite{Warstadt2023}.
    \item \textit{How does the addition of learnable parameters to the activation function improve the performance of the model?} We modified static activation functions to include learnable parameters, pre-trained them, and evaluated them on the BabyLM evaluation pipeline \cite{Warstadt2023}.
    \item \textit{Do FFNs using KAN-networks outperform FFNs using MLP networks?} We used efficient-kan implementation of KAN-networks \cite{efficient-kan}, pre-trained them and evaluated them on BabyLM evaluation pipeline \cite{Warstadt2023}.
\end{itemize}
\subsection{Dataset}
We used the TinyStoreis dataset for pre-training. It's a dataset of short stories, that contains words that a typical 4-year-old would likely understand, generated by GPT-3.5 and GPT-4. It has been shown that they can be used to train LMs that are around 10M parameters and can still generate coherent stories. \cite{Eldan2023}.

\subsection{Models}
We used Hugging face implementations of GPTNeoForCausalLM \cite{huggingfaceNEO} and RobertaForMaskedLM \cite{huggingfaceRoberta} models. GptNEO is GPT2-based decoder model, while roBERTa is based on Google's BERT model from 2018. We used these models as baselines for our experiments. The hyperparameters used for the models can be seen in table \ref{tab:Hyperparameters}. The change in intermediate size for KAN-MLP implementnation was made due to increased number of learnable parameters in the activation function and the need to keep the total number of parameters in the model approximately constant. The final parameter counts shown in table \ref{tab:parameter-counts}.

\subsection{Evaluation Setting and Metrics}
\subsubsection{Evaluation pipeline}
We used the BabyLM evaluation pipeline \cite{Warstadt2023} to evaluate the models. The pipeline consists of three components: BLiMP, GLUE and SuperGLUE. BLiMP is a benchmark for evaluating the linguistic capabilities of language models, consisting of 17 metrics, each specific in syntax, morphology or semantics. Models were evaluated zero-shot, by comparing the probabilities of the sequences in a minimal pair, under the assumption that the acceptable sequence will be considered more likely than its unacceptable counterpart. The final score was computed as an average of those 17 metrics. \cite{Warstadt2023blimp} \cite{warstadt-etal-2023-findings}.  GLUE and SuperGLUE are benchmarks for evaluating the performance of language models on a variety of natural language understanding tasks. GLUE consists of 9 tasks, while SuperGLUE consists of 8 tasks. The final score was computed as the average of the scores 7/9 GLUE tasks and 3/8 SuperGLUE tasks \cite{Wang2019} \cite{Wang2020}.

\subsubsection{Statistical significance testing}
\textbf{TODO: Make this section more reader friendly.} \\
To ensure the results are statistically significant, we trained each model 6 times with diffrent seeds. The following section justifies the choices made for the statistical significance testing, based and summarised from The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing \cite{dror2018hitchhikers}. \\
Since the distribution of the test statistic is not known, we had to choose approaches from non parametric family of approaches. Which is split in two categories. Sampling based tests and sampling free tests. Sampling free tests hold less statistical power, therefore we decided to use sampling based tests.
\\\\
Sampling based tests compensate for the lack of distribution information with resampling. They are computationally more expensive, but can be less effective for smaller sets of results. We used bootstrapping as a sampling based test. It is a resampling method that involves drawing samples with replacement from the original results. These samples are used to approximate the distribution of the statistic. We use that to calculate mean difference between the two models being compared and confidence intervals of the results. We used 10 000 samples for bootstrapping in our experiments. The exact implementation used, available in the replication package [TODO: ref].

\subsection{Hardware}
All the models were trained and evaluated on a single NVIDIA A100 GPU with 4 CPUs and 24GB of memory on DelftBlue cluster.



\begin{table*}[h!]
    \centering
    \begin{tabular}{lrrrrr}
    \hline
    \toprule \textbf{Parameter} & \textbf{GPT Neo} & \textbf{RoBERTa} \\ \hline
    \toprule \textbf{Embedding Parameters} & & \\ \hline
    Vocab Size & 10,000 & 10,000 \\ \hline
    Hidden Size & 512 & 512 \\ \hline
    Max Position Embeddings & 512 & 513 \\ \hline
    \toprule \textbf{Blocks (Attention \& FFN)} & & \\ \hline
    Number of Layers & 2 & 2 \\ \hline
    Attention Types & global and local & global \\ \hline
    Number of Attention Heads & 4 & 4 \\ \hline
    Window Size & 256 & N/A \\ \hline
    Intermediate Size & 1024 \textbf{(256 for KAN-MLP)} & 1024 \textbf{(384 for KAN-MLP)} \\ \hline
    \end{tabular}
    \caption{Comparison of Parameters for GPT Neo and RoBERTa}
    \label{tab:Hyperparameters}
\end{table*}

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|}
    \hline
    \textbf{Model Name} & \textbf{Number of Parameters} \\ \hline
    GPT with GELU (baseline) & 9.0M \\ \hline
    BERT with GELU (baseline) & 9.0M \\ \hline
    BERT with Learnable-GELU & 9.0M \\ \hline
    GPT with Learnable-GELU & 9.0M \\ \hline
    GPT with ReLU & 9.0M \\ \hline
    BERT with ReLU & 9.0M \\ \hline
    BERT with PReLU & 9.0M \\ \hline
    GPT with PReLU & 9.0M \\ \hline
    GPT with SiLU & 9.0M \\ \hline
    BERT with SiLU & 9.0M \\ \hline
    BERT with Swish & 9.0M \\ \hline
    GPT with Swish & 9.0M \\ \hline
    GPT with KAN MLP & 11.0M \\ \hline
    BERT with KAN MLP & 11.0M \\ \hline
    \end{tabular}
    \caption{Model Names and Number of Parameters}
    \label{tab:parameter-counts}
\end{table}