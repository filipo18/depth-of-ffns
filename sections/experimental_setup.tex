% Define the research questions clearly. Discuss the specific configurations you have settled on, and
% elaborate on the data sets you have utilized. Document the evaluation setup and define the metrics.
% ● 4.1 Research Questions
% ● 4.2 Dataset
% ● 4.3 Models
% ● 4.4 Evaluation Setting and Metrics.
% Evaluation Metrics: You should mention the components of the BabyLM evaluation pipeline; but can defer
% explaining each (fine-tune) task in detail to the BLiMP/GLUE/SuperGLUE papers. However, you should
% describe the process each of these components uses to return a score (i.e. how is the final score
% computed?)
% Evaluation Settings: hardware.
% ● 4.5 Configuration and Implementation Details

\section{Experimental setup} % PAST TENSE!!!
\label{sec:experimental_setup}

\subsection{Research questions}
Through the experiment, we aimed to answer the following research questions:
\begin{itemize}
    \item \textit{Is the choice of activation function relevant to the performance of smaller models with 10M parameters?}  We compared the baseline models (with \textsc{GELU} activation) and models with \textit{ReLU} activation function.
    \item \textit{How does the addition of learnable parameters to the activation function improve the performance of the model?} We modified static activation functions \textsc{GELU}, \textsc{SiLU} and \textsc{ReLU} to include learnable parameters.
    \item \textit{Do FFNs using KAN-networks outperform FFNs using MLP networks?} We used efficient-kan implementation\footnotemark[3] of KAN-networks.
\end{itemize}
All of the setups above were evaluated on the BabyLM evaluation pipeline \cite{Warstadt2023}.

\subsection{Models\protect\footnote{\label{footnote:code} The research group supervisor implemented the pre-training and evaluation for the baseline models, which were then individually modified by the research group members.} \protect\footnote{\label{footnote:text} These sections were written based on the guidelines provided by the research group supervisor and discussions with the other group members.}}
We used Hugging face implementations of GPTNeoForCausalLM \cite{huggingfaceNEO} and RobertaForMaskedLM \cite{huggingfaceRoberta} models. GptNEO is a GPT2-based decoder model, while roBERTa is based on Google's BERT model from 2018. We chose these models as they are well-established without and without the newer optimizations found in more recent other architectures, providing simplicity that is more suitable the scope of this project. Additionally, testing on encoder and decoder architectures increases the generalizability the research. GPT-Neo and Roberta also come in pre-trained form, however as we do not have the resources to train them to that extent, we pre-train on our own, smaller dataset so we have a baseline to compare to. The hyperparameters used for the models can be seen in table \ref{tab:Hyperparameters}. The change in intermediate size for KAN-MLP implementation was made due to the increased number of learnable parameters in the activation function and the need to keep the total number of parameters in the model approximately constant. The final parameter count for models using KAN was 11 million, whereas the parameter count for the other models was 9 million.

\subsection{Activation functions}
\textsc{ReLU} and \textsc{SiLU} are provided by used hugging face implementations and can be set via a configuration file. The rest of the activation functions were implemented as described in Section \ref{sec:approach} and are available in the replication package. [todo: cite replication]

\subsection{Dataset\protect\footnotemark[5]}
We used the TinyStories \cite{Eldan2023} dataset for pre-training. It is a dataset of short stories, with a total of 2.1 million samples with an average of 175.4 words per story, containing words that a typical 4-year-old would likely understand, generated by GPT-3.5 and GPT-4. We used them as it has been shown that training on reduced dataset complexity exhibits better natural language understanding than GPT-2 (125M) at a fraction of the training cost.

\subsection{Evaluation Setting and Metrics}
\subsubsection{Evaluation pipeline\protect\footnotemark[4] \protect\footnotemark[5]}
We used the BabyLM evaluation pipeline \cite{Warstadt2023} to evaluate the models. The pipeline consists of three components: BLiMP, GLUE, and SuperGLUE. BLiMP is a benchmark used to evaluate the linguistic capabilities of language models. It consists of pairs of minimally different sentences, with one sentence in the pair containing a grammatical error. Performance is measured by the model's ability to correctly assign a higher likelihood to the grammatically correct sentence. \cite{Warstadt2023blimp} \cite{warstadt-etal-2023-findings}. GLUE and SuperGLUE are benchmarks that assess the performance of language models across a range of tasks focused on text understanding and reasoning. The BabyLM pipeline integrates select tasks from both GLUE and SuperGLUE \cite{Wang2019} \cite{Wang2020}.

\subsubsection{Statistical significance testing}
To ensure the results are statistically significant, we trained each model 6 times with different seeds. The following section justifies the choices made for the statistical significance testing, based on and summarised from The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing \cite{dror2018hitchhikers}. \\
Since the distribution of the test statistic is not known, we had to choose approaches from a nonparametric family of approaches, which is split into two categories. Sampling-based tests and sampling-free tests. Sampling-free tests hold less statistical power, therefore we decided to use sampling-based tests.
\\\\
Sampling-based tests compensate for the lack of distribution information with resampling, which makes them computationally more expensive. We used bootstrapping as a sampling-based test. It is a resampling method that involves drawing samples with replacements from the original results. These samples are used to approximate the distribution of the statistics. We use that to calculate the means and 95\% confidence interval of BLiMP and GLUE scores for each of the models. We used 10 000 samples for bootstrapping in our experiments. The exact implementation used, is available in the replication package [TODO: cite replication].

\subsection{Hardware}
All the models were trained and evaluated on a single NVIDIA A100 GPU with 4 CPUs and 24GB of memory on DelftBlue cluster.



\begin{table*}[h!]
    \centering
    \begin{tabular}{lrrrrr}
    \hline
    \toprule \textbf{Parameter} & \textbf{GPT Neo} & \textbf{RoBERTa} \\ \hline
    \toprule \textbf{Embedding Parameters} & & \\ \hline
    Vocab Size & 10,000 & 10,000 \\ \hline
    Hidden Size & 512 & 512 \\ \hline
    Max Position Embeddings & 512 & 513 \\ \hline
    \toprule \textbf{Blocks (Attention \& FFN)} & & \\ \hline
    Number of Layers & 2 & 2 \\ \hline
    Attention Types & global and local & global \\ \hline
    Number of Attention Heads & 4 & 4 \\ \hline
    Window Size & 256 & N/A \\ \hline
    Intermediate Size & 1024 \textbf{(256 for KAN-MLP)} & 1024 \textbf{(256 for KAN-MLP)} \\ \hline
    \end{tabular}
    \caption{Comparison of Parameters for GPT Neo and RoBERTa}
    \label{tab:Hyperparameters}
\end{table*}