% Define the research questions clearly. Discuss the specific configurations you have settled on, and
% elaborate on the data sets you have utilized. Document the evaluation setup and define the metrics.
% ● 4.1 Research Questions
% ● 4.2 Dataset
% ● 4.3 Models
% ● 4.4 Evaluation Setting and Metrics.
% Evaluation Metrics: You should mention the components of the BabyLM evaluation pipeline; but can defer
% explaining each (fine-tune) task in detail to the BLiMP/GLUE/SuperGLUE papers. However, you should
% describe the process each of these components uses to return a score (i.e. how is the final score
% computed?)
% Evaluation Settings: hardware.
% ● 4.5 Configuration and Implementation Details

\section{Experimental setup}

\subsection{Research questions}
Through the experiment, we aim to answer the following research questions:
\begin{itemize}
    \item \textit{Is the choice of activation function relevant to the performance of smaller models with 10M parameters?}  We will compare the pre-train baselines models and models using \textit{SiLU} and \textit{ReLU} activation functions and compare the results of evaluation on BabyLM evaluation pipeline \cite{Warstadt2023}.
    \item \textit{How does the addition of learnable parameters to the activation function improve the performance of the model?} We will modify static activation functions to include learnable parameters, pre-train them, and evaluate them on the BabyLM evaluation pipeline \cite{Warstadt2023}.
    \item \textit{Do FFNs using KAN-networks outperform FFNs using MLP networks on performance/computational price metric?} We will use efficient-kan implementation of KAN-networks \cite{efficient-kan}, pre-train them and evaluate them on BabyLM evaluation pipeline \cite{Warstadt2023}.
\end{itemize}

\subsection{Dataset}
We use the TinyStoreis dataset for pre-training. It's a dataset of short stories, that contain words that a typical 4-year-old would likely understand, generated by GPT-3.5 and GPT-4. It has been shown that they can be used to train LMs that are around 10M parameters and can still generate coherent stories. \cite{Eldan2023}.

\subsection{Models}
As already mentioned, we use Hugging face implementations of GPTNeoForCausalLM \cite{huggingfaceNEO} and RobertaForMaskedLM \cite{huggingfaceRoberta} models. GptNEO is GPT2- decoder model, while roBERTa is based on Google's BERT model from 2018. We use these models as baselines for our experiments.
\begin{table*}[h!]
    \centering
    \begin{tabular}{|l|c|c|}
    \hline
    \textbf{Parameter} & \textbf{GPT Neo} & \textbf{RoBERTa} \\ \hline
    \textbf{Embedding Parameters} & & \\ \hline
    Vocab Size & 10,000 & 10,000 \\ \hline
    Hidden Size & 512 & 512 \\ \hline
    Max Position Embeddings & 512 & 513 \\ \hline
    \textbf{Blocks (Attention \& FFN)} & & \\ \hline
    Number of Layers & 2 & 2 \\ \hline
    Attention Types & [[["global", "local"], 1]] & N/A \\ \hline
    Number of Attention Heads & 4 & 4 \\ \hline
    Window Size & 256 & N/A \\ \hline
    Intermediate Size & 1024 & 1024 \\ \hline
    \end{tabular}
    \caption{Comparison of Parameters for GPT Neo and RoBERTa}
\end{table*}
\\

\textit{NOTE: Update the table with changes to parameters made in GeGLU and KAN implementations}

The final parameter counts for the implemented models can be seen in the table below:
\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|}
    \hline
    \textbf{Model Name} & \textbf{Number of Parameters} \\ \hline
    BERT-GeGlu & 10.0M \\ \hline
    GPT-GeGlu & 10.0M \\ \hline
    BERT-Learnable-GEGLU & 10.0M \\ \hline
    BERT-Learnable-GELU & 9.0M \\ \hline
    GPT-Learnable-GEGLU & 10.0M \\ \hline
    BERT-Learnable-GELU & 9.0M \\ \hline
    GPT-Learnable-GELU & 9.0M \\ \hline
    GPT-ReLU & 9.0M \\ \hline
    BERT-ReLU & 9.0M \\ \hline
    BERT-Learnable-GELU & 9.0M \\ \hline
    GPT-Learnable-GELU & 9.0M \\ \hline
    BERT-Swish & 9.0M \\ \hline
    GPT-Swish & 9.0M \\ \hline
    BERT-PReLU & 9.0M \\ \hline
    GPT-PReLU & 9.0M \\ \hline
    GPT-KAN & 11.0M \\ \hline
    BERT-KAN & 11.0M \\ \hline
    GPT-SiLU & 9.0M \\ \hline
    BERT-SiLU & 9.0M \\ \hline
    GPT-base & 9.0M \\ \hline
    BERT-base & 9.0M \\ \hline
    \end{tabular}
    \caption{Model Names and Number of Parameters}
\end{table}

\subsection{Evaluation Setting and Metrics}
We use the BabyLM evaluation pipeline \cite{Warstadt2023} to evaluate the models. The pipeline consists of three components: BLiMP, GLUE and SuperGLUE. BLiMP is a benchmark for evaluating the linguistic capabilities of language models, consisting of 17, each specific in syntax, morphology or semantics. Models are evaluated zero-shot, by comparing the probabilities of the sequences in a minimal pair, under the assumption that the acceptable sequence will be considered more likely than its unacceptable counterpart. The final score is computed as an average of those 17 metrics. \cite{Warstadt2023blimp} \cite{warstadt-etal-2023-findings}.  GLUE and SuperGLUE are benchmarks for evaluating the performance of language models on a variety of natural language understanding tasks. GLUE consists of 9 tasks, while SuperGLUE consists of 8 tasks. The final score is computed as the average of the scores 7/9 GLUE tasks and 3/8 SuperGLUE tasks \cite{Wang2019} \cite{Wang2020}.

\subsection{Hardware}
All the models were trained and evaluated on a single NVIDIA A100 GPU with 4 CPUs and 24GB of memory on DelftBlue cluster.