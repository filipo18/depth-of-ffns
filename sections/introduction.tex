
\section{Introduction} % PRESENT TENSE!!!
% ** Motvivate small models **
% - Since transfomer architecture llms are the thing
% - They are getting bigger, but according to current trends we are running out of high quality data where they perform the best \cite{Villalobos2022}
% - Therefore this project studies effect of arcitectural desisions on smaller models

% ** Motivate activation functions + gap **
% - Historically, important, such as move from sigmoind function to relu in state of the art neural networks, which improved speed
% - Development on activation functions cointinued in LLM era 
% - 400 documented functions \cite{Kunc2024}
% - Researchers seem to compare their new activation agaist the previously used one but there seems to be no comprehensive comparison of multiple activation functions under consistent conditions
% - mainly the issues being different datasets and different model sizes 

The transformer architecture \cite{Vaswani2017} has revolutionized the AI landscape and enabled the development of large language models (LLMs). However, as these models continue to grow in size. Current trends are forecasting running out of high-quality data required for optimal performance \cite{Villalobos2022}. This limitation stimulates the initiative to explore the impact of architectural decisions on smaller models, which this project aims to investigate. By understanding how to optimize smaller models, we can ensure the progress of LLMs even with the projected lack of high-quality data.

The choice of activation functions has historically played a crucial role in the advancement of neural networks, such as the shift from Sigmoid activation functions to ReLU (Rectified Linear Unit), which significantly improved training speeds. As the era of large language models (LLMs) unfolded, the development of activation functions continued to evolve, resulting in over 400 documented activation functions \cite{Kunc2024}. Despite this progress, literature typically compares new activation functions against their immediate predecessors, leading to a gap in the literature: there is no comprehensive comparison of multiple activation functions under consistent conditions. This inconsistency is primarily due to variations in datasets and model sizes used in different studies, highlighting the need for a systematic evaluation. More specifically, with the trend of increasing model sizes, the investigation of the impact activation functions have on smaller models was left neglected. This research aims to address this gap by exploring the impact of activation functions on smaller-scale language models with around 10 million parameters.

***(Structure of the paper here?)***