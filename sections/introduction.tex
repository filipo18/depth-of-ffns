
\section{Introduction} % PRESENT TENSE!!!
\label{sec:introduction}
% ** Motvivate small models **
% - Since transfomer architecture llms are the thing
% - They are getting bigger, but according to current trends we are running out of high quality data where they perform the best \cite{Villalobos2022}
% - Therefore this project studies effect of arcitectural desisions on smaller models

% ** Motivate activation functions + gap **
% - Historically, important, such as move from sigmoind function to relu in state of the art neural networks, which improved speed
% - Development on activation functions cointinued in LLM era 
% - 400 documented functions \cite{Kunc2024}
% - Researchers seem to compare their new activation agaist the previously used one but there seems to be no comprehensive comparison of multiple activation functions under consistent conditions
% - mainly the issues being different datasets and different model sizes 

% Quick setup of the experiment

The transformer architecture \cite{Vaswani2017} has revolutionized the AI landscape and enabled the development of commercial large language models (LLMs) like ChatGPT. However, as these models continue to grow in size, current trends forecast running out of high-quality data required for optimal performance \cite{Villalobos2022}. This limitation stimulates the initiative to improve sample efficiency motivated by the observation that LLMs are exposed to orders of magnitude more information than a human in their lifetime, yet are certainly not leveraging all this information nearly as efficiently. Therefore, this research aims to investigate the impact of architectural decisions on smaller models which are often left neglected. By understanding how to optimize smaller models, we can ensure the progress of LLMs even with the projected lack of high-quality data. Furthermore, smaller models can be deployed on edge devices that can ensure privacy and be used for spellcheck, predictive typing, conversational assistance, etc.

The activation function in a neural network determines if a neuron should be activated or not. The choice of activation functions has historically played a crucial role in the advancement of neural networks, such as the shift from Sigmoid activation functions to \textsc{ReLU} (Rectified Linear Unit), which simplified computation, improved feature learning, and mitigated vanishing gradient problem \cite{nair2010rectified}. As the era of LLMs unfolded, the development of activation functions continued to evolve, resulting in over 400 documented activation functions \cite{Kunc2024}. Despite this progress, literature typically compares new activation functions against their immediate predecessors and usually uses all the latest state-of-the-art configurations (bigger and bigger models) leading to a gap in the literature: there is no comprehensive comparison of multiple activation functions under consistent model sizes, furthermore, with the trend of increasing model sizes, the investigation of the impact activation functions have on smaller models is neglected. This research aims to address the aforementioned gap by exploring the impact of existing and novel activation functions on smaller-scale language models with approximately 10 million (10M) parameters. 

For purposes of the research, we will modify Hugging face implementations of \textsc{GPTNeo} \cite{huggingfaceNEO} and \textsc{RoBERTa} \cite{huggingfaceRoberta} with selected existing activation functions and a novel Learnable GELU activation function, set the hyperparameters to amount to a total size of approximately 10M trainable parameters and evaluate them on the BabyLM evaluation pipeline \cite{Warstadt2023}. More details on the setup are provided in section \ref{sec:experimental_setup}.

\textbf{ToDo: Contributions.}

\noindent Our contributions are as follows:

\begin{itemize}

    \item todo 
    \item todo 
    \item todo 
    \item A replication package\footnote{https://github.com/AISE-TUDelft/tiny-transformers} for reproducing our findings, and our models\footnote{https://huggingface.co/collections/AISE-TUDelft/brp-tiny-transformers-666c352b3b570f44d7d2a519} published on HuggingFace.

\end{itemize}