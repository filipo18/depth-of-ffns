
\section{Introduction} % PRESENT TENSE!!!
\label{sec:introduction}
% ** Motvivate small models **
% - Since transfomer architecture llms are the thing
% - They are getting bigger, but according to current trends we are running out of high quality data where they perform the best \cite{Villalobos2022}
% - Therefore this project studies effect of arcitectural desisions on smaller models

% ** Motivate activation functions + gap **
% - Historically, important, such as move from sigmoind function to relu in state of the art neural networks, which improved speed
% - Development on activation functions cointinued in LLM era 
% - 400 documented functions \cite{Kunc2024}
% - Researchers seem to compare their new activation agaist the previously used one but there seems to be no comprehensive comparison of multiple activation functions under consistent conditions
% - mainly the issues being different datasets and different model sizes 

% Quick setup of the experiment

The transformer architecture \cite{Vaswani2017} has revolutionized the AI landscape and enabled the development of commerical large language models (LLMs) like ChatGPT. However, as these models continue to grow in size, current trends forcast running out of high-quality data required for optimal performance \cite{Villalobos2022}. This limitation stimulates the initiative to improve sample efficiency motivated by the observation that LLMs are exposed to orders of magnitude more information than a human in their lifetime, yet are certainly not leveraging all this information nearly as efficiently. Thereore, this reserachs aims to investiate the impact of architectural decisions on smaller models which are often left neglected. By understanding how to optimize smaller models, we can ensure the progress of LLMs even with the projected lack of high-quality data. Furthermore, smaller models can be deployed on edge devices can insure privacy and be used for spellcheck, predictive typing, conversational assistance etc.

The activation function in a nerual netowrk, determines if neuron sould be activated or not. The choice of activation functions has historically played a crucial role in the advancement of neural networks, such as the shift from Sigmoid activation functions to \textsc{ReLU} (Rectified Linear Unit), which simplified computation, improved feature learning and mitigated vanishing gradient problem \cite{nair2010rectified}. As the era of LLMs unfolded, the development of activation functions continued to evolve, resulting in over 400 documented activation functions \cite{Kunc2024}. Despite this progress, literature typically compares new activation functions against their immediate predecessors and usually using all the latest state-of-the-art configureation (bigger and bigger models) leading to a gap in the literature: there is no comprehensive comparison of multiple activation functions under consistent model sizes, furthermore, with the trend of increasing model sizes, the investigation of the impact activation functions have on smaller models is neglected. This research aims to address the aforementioned gap by exploring the impact of existing and novel activation functions on smaller-scale language models with approximately 10 million (10M) parameters. 

For purpuses of the research, we will modify Hugging face implementations of \textsc{GPTNeo} \cite{huggingfaceNEO} and \textsc{RoBERTa} \cite{huggingfaceRoberta} with selected existing activation functions and a novel Learnable GELU activation function, set the hyperparameters to amount to a total size of approximately 10M trainable parameters and evaluate them on the BabyLM evaluation pipeline \cite{Warstadt2023}. More details on the setup provided in section \ref{sec:experimental_setup}.

This paper provides the evolution and bacground of activation functions in section \ref{sec:background} and the approach of the investiation in section \ref{sec:approach}. It describes the experimental setup using in section \ref{sec:experimental_setup}. The results are presented and analysed in section \ref{sec:results}, and the discussion addresses performance outcomes and future research directions in section \ref{sec:discussion}. Conslusion and responsible research considerations are presented in sections \ref{sec:conclusion} and \ref{sec:responsible_research}, respectively.