# Todo List
- [x] Set up latex, git repo and overleaf
- [x] finish watching 3blue1brown series and reading https://jalammar.github.io/illustrated-transformer/
- [x] read attention is all you need
- [x] read tiny stories, baby lm challange
- [x] read ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models
- [x] search for more papers related to FFNs
- [ ] write introduction of the report
- [ ] write related works of the report
- [x] skim through papers in zotero common folder
- [x] make research plan draft 
- [ ] make final research plan
- [ ] (optioal) start coding alongside Karpathy videos (changed for https://www.gptandchill.ai/codingproblems)

# Week 1 summary 
- watched 3blue1brown neural networks series
- wached Language Modeling series from vcubingx recommended by 3blue1brown
- spent too much time going through bunch of other youtube videos to get better high level understanding of transformer
- started watching Karpathy, but didn't understand a lot of concepts
- to get up to speed, worked practice problems of coding GPT from scratch https://www.gptandchill.ai/codingproblems
- decided strat with activation functions (lost cause :D)
- collected papers on actications functions
- skimed through Three Decades of Activations: A Comprehensive Survey of 400 Activation Functions for Neural Networks to get more ideas
- read throug Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark
- read through Activation Functions: Comparison of Trends in Practice and Research for Deep Learning
- read throguh Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks

# Week 1 ToDo (friday to sunday)
- [ ] finish and submit research plan
- [ ] make todo for week 2