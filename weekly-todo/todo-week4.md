# Week 3 sumary
- [x] why is no one using adaptive activation functions in LLM?
- [] try to implementent LLM using adaptive activation function (swish and prelu) (implementing KAN instead)
- [] reimplement MLP network into KAN network (bunch of errors with shit KAN library, continue in week 4)
- [x] read and prepare for KAN paper presentation
- [x] write related works section of the reprot
- [x] ACS assigment 1
- [] write approach setion (moved to week 4)
- [x] implement and train NEO with ReLU, GeLu, GeGLu, SiLU (training)
- [x] implement and train BERT with ReLU, GeLu, GeGLu, SiLU (training)
- [] implement neo with ppost norm and normformer? Not doing normalizations

# ToDo week 4
- [] ACS author instructions
- [] ACS poster 
- [] ACS session - Monday, online 13 45
- [] prepare presentation
- [] run evaluation on trained models
- [] implement and train swish and parametric relu
- [] finish implementing with KAN network
- [] start surverying interpretability literature (check zotero common for ideas, start from tiny stories)
- [] read Transformers with Learnable Activation Functions
- [] read The Low-Rank Simplicity Bias in Deep Networks (?)