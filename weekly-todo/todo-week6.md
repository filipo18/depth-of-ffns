# Recap week 5
- [x] prepare presentation
- [x] run evaluation on trained models (evaluation working, but did not finish due to running out of space on the delftblue)
- [x] implement and train swish and parametric relu
- [x] start surverying interpretability literature (check zotero common for ideas, start from tiny stories) (not part of the project anymore)
- [x] read The Low-Rank Simplicity Bias in Deep Networks (?) (not part of the project anymore)
- [x] implement and train geglu
- [x] get evaluation working
- [x] make sure all models are implemented and trained

# ToDo week 6
- [x] fix the space issue on delft blue finish evaluations
- [x] implemenet and train parametric GEGLU and GELU
- [x] work on draft V1, write results, start discusion, revise the other sections
- [x] start surverying interpretability literature (check zotero common for ideas, start from tiny stories) (not part of the project anymore)
- [x] read The Low-Rank Simplicity Bias in Deep Networks (?) (not part of the project anymore)
- [x] finish evals for static activations, swish, prelu, kan
- [x] train learnable/parametric GELU with 5 new seeds
- [ ] implement and train learnable/parametric GEGLU with 5 new seeds (moved to next week)